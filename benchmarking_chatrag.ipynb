{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2539422-2a2a-4f13-9cc6-66e110a5f3f7",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca80ac-625d-4011-8e8e-7e518ac41bbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb7438b-b6dd-4b78-bb8b-5dd68bd1a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829723d9-b41b-48ab-a1ab-d829353230a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nvidia/Llama3-ChatQA-1.5-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b79e844-00d8-4254-8b2d-1bf978501df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d19702-098f-40d2-9bc5-b0d49faeec88",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "- All data have been downloaded, we are just loading it from disk to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00909108-10bb-4eb5-b7f1-bfbec9394003",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = dataset_names[0] #choose\n",
    "dataset = load_dataset(\"nvidia/ChatRAG-Bench\", dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f07a43a-fdec-4783-99f9-2cda64f7af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_question(turn_list, dataset_name):\n",
    "\n",
    "    ## only take the lastest 7 turns\n",
    "    turn_list = turn_list[-7:]\n",
    "    assert turn_list[-1]['role'] == 'user'\n",
    "\n",
    "    long_answer_dataset_list = [\"doc2dial\", \"quac\", \"qrecc\", \"inscit\", \"doqa_movies\", \"doqa_travel\", \"doqa_cooking\", \"hybridial\", \"convfinqa\"]\n",
    "    long_and_short_dataset_list = [\"topiocqa\"]\n",
    "    entity_dataset_list = [\"sqa\"]\n",
    "    short_dataset_list = [\"coqa\"]\n",
    "\n",
    "    if dataset_name in long_answer_dataset_list:\n",
    "        for item in turn_list:\n",
    "            if item['role'] == 'user':\n",
    "                ## only needs to add it on the first user turn\n",
    "                item['content'] = 'Please give a full and complete answer for the question. ' + item['content']\n",
    "                break\n",
    "    \n",
    "    elif dataset_name in long_and_short_dataset_list:\n",
    "        turn_list[-1]['content'] = \"Answer the following question with a short span, or a full and complete answer. \" + turn_list[-1]['content']\n",
    "\n",
    "    elif dataset_name in entity_dataset_list:\n",
    "        turn_list[-1]['content'] = \"Answer the following question with one or a list of items. \" + turn_list[-1]['content']\n",
    "\n",
    "    elif dataset_name in short_dataset_list:\n",
    "        turn_list[-1]['content'] = \"Answer the following question with a short span. The answer needs to be just in a few words. \" + turn_list[-1]['content']\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"please input a correct dataset name!\")\n",
    "    \n",
    "    question = \"\"\n",
    "    for item in turn_list:\n",
    "        if item[\"role\"] == \"user\":\n",
    "            question += \"User: \" + item[\"content\"] + \"\\n\\n\"\n",
    "        else:\n",
    "            assert item[\"role\"] == \"assistant\"\n",
    "            question += \"Assistant: \" + item[\"content\"] + \"\\n\\n\"\n",
    "    \n",
    "    question += \"Assistant:\"\n",
    "    \n",
    "    return question\n",
    "\n",
    "\n",
    "def get_inputs(example, dataset_name, tokenizer, num_ctx, max_output_len, max_seq_length=4096):\n",
    "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "\n",
    "    turn_list = example['messages']\n",
    "    question_formatted = reformat_question(turn_list, dataset_name)\n",
    "\n",
    "    ctx_list = [\"title: \" + ctx[\"title\"] + \", source: \" + ctx[\"text\"] for ctx in example['ctxs'][:num_ctx]]\n",
    "    context = \"\\n\\n\".join(ctx_list)\n",
    "\n",
    "    context_tokens = tokenizer.encode(context)\n",
    "    question_tokens = tokenizer.encode(question_formatted)\n",
    "    system_tokens = tokenizer.encode(system)\n",
    "\n",
    "    if len(context_tokens) + len(question_tokens) + len(system_tokens) + max_output_len >= max_seq_length:\n",
    "        context_tokens = context_tokens[:max_seq_length - max_output_len - len(question_tokens) - len(system_tokens)]\n",
    "        context = tokenizer.decode(context_tokens, skip_special_tokens=True)\n",
    "\n",
    "    model_input = system + \"\\n\\n\" + context + \"\\n\\n\" + question_formatted\n",
    "\n",
    "    return {\"model_input\": model_input}\n",
    "\n",
    "def process_dataset(dataset, dataset_name, tokenizer, num_ctx, max_output_len, max_seq_length=4096):\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda example: get_inputs(example, dataset_name, tokenizer, num_ctx, max_output_len, max_seq_length),\n",
    "        batched=False,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124ec2ec-a279-4a55-8f54-4207ee9b0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7983/7983 [00:13<00:00, 612.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_ctx = 5  # Specify the number of contexts to use\n",
    "max_output_len = 64  # Specify the maximum output length\n",
    "max_seq_length = 64  # Specify the maximum sequence length\n",
    "\n",
    "processed_dataset = process_dataset(dataset['dev'], dataset_name, tokenizer, num_ctx, max_output_len, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f3cd1-32aa-41c7-9a3a-c9fc773e8418",
   "metadata": {},
   "source": [
    "### Sampling Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60cf8937-d5d4-4814-9df7-ae99af909e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n",
      "\n",
      "title:, source: Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that\n",
      "\n",
      "User: What color was Cotton?\n",
      "\n",
      "Assistant: white\n",
      "\n",
      "User: Answer the following question with a short span. The answer needs to be just in a few words. Where did she live?\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset[1]['model_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02965f0-7097-46a8-a318-0612f7e980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n",
      "\n",
      "title:, source: Las Vegas (, Spanish for \"The Meadows\"), officially the City of Las Vegas and often known simply as Vegas, is the 28th-most populated city in the United States, the most populated city in the state of Nevada, and the county seat of Clark County. The city anchors the Las Vegas Valley metropolitan area and is the largest city within the greater Mojave Desert. Las Vegas is an internationally renowned major resort city, known primarily for its gambling, shopping, fine dining, entertainment, and nightlife. The Las Vegas Valley as a whole serves as the leading financial, commercial, and cultural center\n",
      "\n",
      "User: Which state is it in?\n",
      "\n",
      "Assistant: Nevada\n",
      "\n",
      "User: Is it located in a desert?\n",
      "\n",
      "Assistant: Yes\n",
      "\n",
      "User: what is the name of the desert?\n",
      "\n",
      "Assistant: Mojave Desert.\n",
      "\n",
      "User: Answer the following question with a short span. The answer needs to be just in a few words. is it a small city?\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset[-1]['model_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54643d8c-535d-40e3-b5f2-1d33f42f3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_dataset is python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1afd5781-0cb6-4175-b9be-f4dd3492ca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model_input'],\n",
       "    num_rows: 7983\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "686a0f7b-734e-4ca1-85ea-0378303d4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model is already downloaded as we have added it the image creation process, \n",
    "#the time taken to run the cell is due to loading of model from disk to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9d3edc-f605-41cb-9681-8f1694e24aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-05 20:02:04 llm_engine.py:73] Initializing an LLM engine with config: model='nvidia/Llama3-ChatQA-1.5-8B', tokenizer='nvidia/Llama3-ChatQA-1.5-8B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 06-05 20:02:04 tokenizer.py:32] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-05 20:02:31 llm_engine.py:222] # GPU blocks: 27144, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "model_vllm = LLM(model_id)\n",
    "sampling_params = SamplingParams(temperature=0, top_k=1, max_tokens= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68fe39b1-7d1f-428e-a015-3eb97af9d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 7983/7983 [01:33<00:00, 85.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.32302021980286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## This changes the GPU support to 8\n",
    "tik = time.time()\n",
    "## bos token for llama-3\n",
    "bos_token = \"<|begin_of_text|>\"\n",
    "output_list = []\n",
    "prompts = []\n",
    "for example in processed_dataset['model_input']:\n",
    "    prompt = bos_token + example\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = model_vllm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    #print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "    # print(\"generated_text:\", generated_text)\n",
    "    output_list.append(generated_text)\n",
    "tok = time.time()\n",
    "print(tok - tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c4bd120-08ba-434b-b685-73c05f8074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_datapath = os.path.join(f\"{dataset_name}_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23cd7cfc-a9f0-4c7d-8fa8-ecce07d944bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to coqa_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"writing to %s\" % output_datapath)\n",
    "with open(output_datapath, \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Generated Text\"])  # Write the header row\n",
    "    for output in output_list:\n",
    "        csv_writer.writerow([output])  # Write each generated text as a row in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf42d65-4793-4434-a060-f63def9b96a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__  chatrag.ipynb  coqa_output.csv  rag_bench.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e01a32-6933-49bb-ba12-ae2ee9fe51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  5 20:05:56 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             113W / 700W |  72973MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0a71-1653-40e6-ac02-48364ced293a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
